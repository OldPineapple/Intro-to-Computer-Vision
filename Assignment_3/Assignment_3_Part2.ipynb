{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_3_Part2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM7nGstqRGHRtbg2AyI3QG7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tlu4u7Eg9DoS"},"source":["#ECSE 415 Assignment 3 Part 2\n","Hanwen Wang 260778557"]},{"cell_type":"code","metadata":{"id":"aGoCC7RN888L","executionInfo":{"status":"ok","timestamp":1604136068201,"user_tz":240,"elapsed":19960,"user":{"displayName":"王翰文","photoUrl":"","userId":"05073677028121749533"}},"outputId":"6ece336f-bb1d-453d-b372-0e59eb19513d","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# make path = './' connect to the path of the folder stroing images\n","path = '/content/drive/My Drive/ECSE_415_F_2020/Assignment_3/'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x3-EIPje-HqS"},"source":["# 2. Image Classfication with Convolution Neural Network (CNN)"]},{"cell_type":"markdown","metadata":{"id":"l7wMlOhd-nUU"},"source":["## 2.1"]},{"cell_type":"code","metadata":{"id":"KMXiur-8AXuK","executionInfo":{"status":"ok","timestamp":1604136084550,"user_tz":240,"elapsed":321,"user":{"displayName":"王翰文","photoUrl":"","userId":"05073677028121749533"}}},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","#normalize the value between -1 to 1\n","transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5], [0.5])])\n","\n","trainset = torchvision.datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,shuffle=True,num_workers=2)\n","\n","testset = torchvision.datasets.MNIST(root='./data',train=False,download=True,transform=transform)\n","testloader = torch.utils.data.DataLoader(testset,batch_size=32,shuffle=False,num_workers=2)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h1OFj2Tv8aou"},"source":["## 2.2"]},{"cell_type":"code","metadata":{"id":"dStBoEXzFNAw","executionInfo":{"status":"ok","timestamp":1604136088186,"user_tz":240,"elapsed":336,"user":{"displayName":"王翰文","photoUrl":"","userId":"05073677028121749533"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        #convolution layer\n","        #nn.Con2d(input channel,output channel(kernel),size), \n","        #size is [,], can be simplified to a number if two numbers are equal\n","        self.conv1 = nn.Conv2d(1, 32, 3)\n","        self.conv2 = nn.Conv2d(32, 64, 3)\n","        self.conv3 = nn.Conv2d(64, 64, 3)\n","        #maxpool layer\n","        #nn.MaxPool2d(kernel size)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        #linear layer\n","        #nn.Linear(input neurons/size,output neurons/size)\n","        self.fc = nn.Linear(4096, 10)\n","    def forward(self, x):\n","        #A convolution layer with 32 kernels of size 3×3 and A ReLU activation\n","        x = F.relu(self.conv1(x))\n","        #A convolution layer with 64 kernels of size 3×3, A ReLU activation and \n","        #A maxpool layer with kernels of size 2×2\n","        x = self.pool(F.relu(self.conv2(x)))\n","        #A convolution layer with 64 kernels of size 3×3 and A ReLU activation\n","        x = F.relu(self.conv3(x))\n","        #A convolution layer with 64 kernels of size 3×3 and A ReLU activation\n","        x = F.relu(self.conv3(x))\n","        #vectorize into a long feature vector, a flattening layer\n","        x = x.view(-1,4096)\n","        #A Linear layer with output size of 10\n","        x = self.fc(x)\n","        return x\n","\n","net = Net()"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVRpj5P2HSK7"},"source":["## 2.3"]},{"cell_type":"code","metadata":{"id":"QiASX2YCHW98","executionInfo":{"status":"ok","timestamp":1604136101020,"user_tz":240,"elapsed":10402,"user":{"displayName":"王翰文","photoUrl":"","userId":"05073677028121749533"}},"outputId":"79efe8fd-72ec-4948-dfc6-96be11e6f7c4","colab":{"base_uri":"https://localhost:8080/"}},"source":["import torch.optim as optim\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","net = net.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N_oKd7ApHTgd"},"source":["## 2.4"]},{"cell_type":"code","metadata":{"id":"O3L98P58LjU0","executionInfo":{"status":"ok","timestamp":1604136259251,"user_tz":240,"elapsed":155530,"user":{"displayName":"王翰文","photoUrl":"","userId":"05073677028121749533"}},"outputId":"dd3a9f21-139b-41c7-a9e7-ed497df8b900","colab":{"base_uri":"https://localhost:8080/"}},"source":["for epoch in range(10):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 300 == 299:    # print every 300 mini-batches, 1800+ in total in one epoch\n","            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 300))\n","            running_loss = 0.0\n","\n","print('Finished Training')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[1,   300] loss: 2.297\n","[1,   600] loss: 2.282\n","[1,   900] loss: 2.254\n","[1,  1200] loss: 2.175\n","[1,  1500] loss: 1.802\n","[1,  1800] loss: 0.872\n","[2,   300] loss: 0.530\n","[2,   600] loss: 0.474\n","[2,   900] loss: 0.418\n","[2,  1200] loss: 0.407\n","[2,  1500] loss: 0.395\n","[2,  1800] loss: 0.392\n","[3,   300] loss: 0.365\n","[3,   600] loss: 0.371\n","[3,   900] loss: 0.356\n","[3,  1200] loss: 0.369\n","[3,  1500] loss: 0.369\n","[3,  1800] loss: 0.344\n","[4,   300] loss: 0.354\n","[4,   600] loss: 0.326\n","[4,   900] loss: 0.309\n","[4,  1200] loss: 0.318\n","[4,  1500] loss: 0.319\n","[4,  1800] loss: 0.315\n","[5,   300] loss: 0.301\n","[5,   600] loss: 0.292\n","[5,   900] loss: 0.284\n","[5,  1200] loss: 0.296\n","[5,  1500] loss: 0.290\n","[5,  1800] loss: 0.281\n","[6,   300] loss: 0.267\n","[6,   600] loss: 0.252\n","[6,   900] loss: 0.263\n","[6,  1200] loss: 0.249\n","[6,  1500] loss: 0.244\n","[6,  1800] loss: 0.236\n","[7,   300] loss: 0.215\n","[7,   600] loss: 0.229\n","[7,   900] loss: 0.220\n","[7,  1200] loss: 0.215\n","[7,  1500] loss: 0.198\n","[7,  1800] loss: 0.187\n","[8,   300] loss: 0.182\n","[8,   600] loss: 0.192\n","[8,   900] loss: 0.172\n","[8,  1200] loss: 0.170\n","[8,  1500] loss: 0.153\n","[8,  1800] loss: 0.157\n","[9,   300] loss: 0.151\n","[9,   600] loss: 0.158\n","[9,   900] loss: 0.154\n","[9,  1200] loss: 0.140\n","[9,  1500] loss: 0.132\n","[9,  1800] loss: 0.118\n","[10,   300] loss: 0.127\n","[10,   600] loss: 0.120\n","[10,   900] loss: 0.121\n","[10,  1200] loss: 0.125\n","[10,  1500] loss: 0.113\n","[10,  1800] loss: 0.119\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WPiKnnvbHUsw"},"source":["## 2.5"]},{"cell_type":"code","metadata":{"id":"XAlA8u5ghpdN","executionInfo":{"status":"ok","timestamp":1604136303767,"user_tz":240,"elapsed":2526,"user":{"displayName":"王翰文","photoUrl":"","userId":"05073677028121749533"}},"outputId":"431ac09b-1841-4e9f-b353-240aef4d0f45","colab":{"base_uri":"https://localhost:8080/"}},"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on test images: %d %%' % (100 * correct / total))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Accuracy of the network on test images: 97 %\n"],"name":"stdout"}]}]}